{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b74d9c9b",
      "metadata": {
        "id": "b74d9c9b"
      },
      "outputs": [],
      "source": [
        "class bcolors:\n",
        "    OK = '\\033[92m' #GREEN\n",
        "    WARNING = '\\033[93m' #YELLOW\n",
        "    FAIL = '\\033[91m' #RED\n",
        "    RESET = '\\033[0m' #RESET COLOR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e210a4af",
      "metadata": {
        "id": "e210a4af"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler, Sampler, SubsetRandomSampler\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "from torchvision import datasets, models\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "from math import ceil\n",
        "import seaborn as sn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2ee7638",
      "metadata": {
        "id": "b2ee7638"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, classification_report, balanced_accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf0c0ceb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf0c0ceb",
        "outputId": "e00215c8-de45-4287-a0e0-0d5e74765b6d"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'{bcolors.OK}{device}{bcolors.RESET}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b70950b3",
      "metadata": {
        "id": "b70950b3"
      },
      "outputs": [],
      "source": [
        "# set the attempt number in attempt\n",
        "# # set the model name in model\n",
        "# set the model version in version\n",
        "\n",
        "hyper_parameter = {\n",
        "    \"attempt\": 'image200-custom-sampler',\n",
        "    \"model\": 'efficientnet',\n",
        "    \"version\": 'b0',\n",
        "    \"learning_rate\": 0.01,\n",
        "    \"batch_size\": 8,\n",
        "    \"num_workers\": 2,\n",
        "    \"no_epochs\": 10,\n",
        "    \"image_size\": 224, \n",
        "    \"in_channels\": 3, \n",
        "    \"num_classes\": 8,\n",
        "    \"load_model\": True\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Dc_Gb4uAPxso",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dc_Gb4uAPxso",
        "outputId": "c3f42d65-c8f9-4442-9736-d724dbeee581"
      },
      "outputs": [],
      "source": [
        "# set data directory\n",
        "data_dir = '../Data/ISIC2019/images200/'\n",
        "sets = ['train', 'test']\n",
        "# to use checkpoint saving create a directory named \"checkpoint\" and a sub directory in the name of the model\n",
        "checkpoint_path = f'./checkpoints/{hyper_parameter[\"model\"]}/{hyper_parameter[\"model\"]}{hyper_parameter[\"version\"]}-{hyper_parameter[\"attempt\"]}'\n",
        "\n",
        "\n",
        "print(f'{bcolors.OK}Enviroment setup complete ðŸ˜ŠðŸ¼!{bcolors.RESET}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YwKgZKQa4ptY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwKgZKQa4ptY",
        "outputId": "d3a69894-66c9-4c7c-f419-a562d5145187"
      },
      "outputs": [],
      "source": [
        "classes = ['AK', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'SCC', 'VASC']\n",
        "\n",
        "train_class_frequency = []\n",
        "for classname in iter(classes):\n",
        "    train_class_frequency.append(len(os.listdir(os.path.join(data_dir, 'train', classname))))\n",
        "\n",
        "image_frequency = pd.DataFrame(columns=sets, index=classes)\n",
        "for dataset in iter(sets):\n",
        "    for classname in iter(classes):\n",
        "        image_frequency[dataset][classname] = len(os.listdir(os.path.join(data_dir, dataset, classname)))\n",
        "image_frequency.index = image_frequency.index.rename('classes')\n",
        "\n",
        "print(image_frequency.to_markdown())\n",
        "print(image_frequency.sum(axis=0))\n",
        "\n",
        "image_ratio = pd.DataFrame(columns=sets, index=classes)\n",
        "for dataset in iter(sets):\n",
        "    for classname in iter(classes):\n",
        "        image_ratio[dataset][classname] = (image_frequency[dataset][classname] / image_frequency[dataset].sum())*100\n",
        "image_ratio.index = image_frequency.index.rename('classes')\n",
        "print(image_ratio.to_markdown())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c110bfbd",
      "metadata": {
        "id": "c110bfbd"
      },
      "outputs": [],
      "source": [
        "class Transforms:\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, img, *args, **kwargs):\n",
        "        return self.transforms(image=np.array(img))['image']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a3c509d",
      "metadata": {
        "id": "4a3c509d"
      },
      "outputs": [],
      "source": [
        "# set the mean and std based on the model documentation\n",
        "# for efficient net bo - pytorch documentation\n",
        "mean=[0.485, 0.456, 0.406]\n",
        "std=[0.229, 0.224, 0.225]\n",
        "# hyper_parameter['image_size']=32\n",
        "\n",
        "data_transforms = {\n",
        "    'train': A.Compose([A.Resize(hyper_parameter['image_size'] , hyper_parameter['image_size']), A.Normalize(mean, std), ToTensorV2()]),\n",
        "    'test': A.Compose([A.Resize(hyper_parameter['image_size'] , hyper_parameter['image_size']), A.Normalize(mean, std), ToTensorV2()])\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39062b80",
      "metadata": {},
      "outputs": [],
      "source": [
        "# create dataset and dataloader\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), transform =Transforms(transforms=data_transforms[x])) for x in sets}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e57ce4fa",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ratio_of=hyper_parameter['batch_size']\n",
        "ratio_of = len(image_datasets['train'])\n",
        "# if we consider trainset (0.80) to be 10,360 the total dataset 12,950 (1.00)\n",
        "# test would be 2,590 (0.20)\n",
        "# ratio_of = 400\n",
        "class_ratio=0.125\n",
        "targets = np.array(image_datasets['train'].targets)\n",
        "ak_idxs = np.where(targets==0)[0]\n",
        "bcc_idxs = np.where(targets==1)[0]\n",
        "bkl_idxs = np.where(targets==2)[0]\n",
        "df_idxs = np.where(targets==3)[0]\n",
        "mel_idxs = np.where(targets==4)[0]\n",
        "nv_idxs = np.where(targets==5)[0]\n",
        "scc_idxs = np.where(targets==6)[0]\n",
        "vasc_idxs = np.where(targets==7)[0]\n",
        "ak = np.random.choice(ak_idxs, int(ratio_of * class_ratio), replace=True)\n",
        "bcc = np.random.choice(bcc_idxs, int(ratio_of * class_ratio), replace=True)\n",
        "bkl = np.random.choice(bkl_idxs, int(ratio_of * class_ratio), replace=True)\n",
        "df = np.random.choice(df_idxs, int(ratio_of * class_ratio), replace=True)\n",
        "mel = np.random.choice(mel_idxs, int(ratio_of * class_ratio), replace=False)\n",
        "nv = np.random.choice(nv_idxs, int(ratio_of * class_ratio), replace=True)\n",
        "scc = np.random.choice(scc_idxs, int(ratio_of * class_ratio), replace=True)\n",
        "vasc = np.random.choice(vasc_idxs, int(ratio_of * class_ratio), replace=True)\n",
        "idxs = np.hstack([ak, bcc, bkl, df, mel, nv, scc, vasc])\n",
        "# np.random.shuffle(idxs)\n",
        "np.random.shuffle([ak, bcc, bkl, df, mel, nv, scc, vasc])\n",
        "\n",
        "print(len(ak))\n",
        "print(len(idxs)==len(ak)*8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e31e89f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1745.5\n",
            "[2.013264129181084, 0.5252783629250677, 0.6652057926829268, 7.303347280334728, 0.3860017691287041, 0.1355728155339806, 2.7794585987261144, 6.899209486166008]\n"
          ]
        }
      ],
      "source": [
        "class_counts = train_class_frequency\n",
        "class_weights = np.zeros_like(len(image_datasets['train']), dtype=np.float16)\n",
        "median_freq = np.median(class_counts)\n",
        "print(median_freq)\n",
        "class_weights = [median_freq/c for c in class_counts]\n",
        "print(class_weights)\n",
        "\n",
        "sample_weights = [0] * len(image_datasets['train'])\n",
        "for idx, (data, label) in enumerate(image_datasets['train']):\n",
        "    class_weight = class_weights[label]\n",
        "    sample_weights[idx] = class_weight\n",
        "\n",
        "medianSampler = WeightedRandomSampler(sample_weights, sum(train_class_frequency))\n",
        "\n",
        "for w in class_weights:\n",
        "    print(f'{(w*100):2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dcb7edc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9.722467\n",
            "2.536677\n",
            "3.212416\n",
            "35.269368\n",
            "1.864082\n",
            "0.654709\n",
            "13.422578\n",
            "33.317703\n"
          ]
        }
      ],
      "source": [
        "# ensure an equal number of representatives from each class in each batch\n",
        "# The class_counts variable is a list that contains the number of samples in each class. \n",
        "# The weights variable is then created by taking the reciprocal of each class count and normalizing the values so that they sum to 1. \n",
        "# This gives each class an equal probability of being selected. Then the WeightedRandomSampler is instantiated with the weights and the number of samples. \n",
        "# Finally, the DataLoader is instantiated with the dataset, the desired batch size, and the sampler.\n",
        "\n",
        "class_counts = train_class_frequency # list of number of samples per class\n",
        "weights = [1/c for c in class_counts]\n",
        "weights = [w/sum(weights) for w in weights]\n",
        "sample_weights = [0] * len(image_datasets['train'])\n",
        "\n",
        "for idx, (data, label) in enumerate(image_datasets['train']):\n",
        "    class_weight = weights[label]\n",
        "    sample_weights[idx] = class_weight\n",
        "\n",
        "weightedSampler = WeightedRandomSampler(sample_weights, sum(train_class_frequency))\n",
        "\n",
        "for w in weights:\n",
        "    print(f'{(w*100):2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf03411a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# oversampling\n",
        "# setting weights to each sample based on class population\n",
        "class_weights = []\n",
        "for root, subdir, files in os.walk(os.path.join(data_dir, \"test\")):\n",
        "    if len(files) > 0:\n",
        "        class_weights.append(1/len(files))\n",
        "\n",
        "sample_weights = [0] * len(image_datasets['test'])\n",
        "\n",
        "for idx, (data, label) in enumerate(image_datasets['test']):\n",
        "    class_weight = class_weights[label]\n",
        "    sample_weights[idx] = class_weight\n",
        "\n",
        "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "class_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6ffd1e8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# if we consider trainset (0.80) to be 10,360 the total dataset 12,950 (1.00)\n",
        "# test would be 2,590 (0.20)\n",
        "class RatioSampler(Sampler):\n",
        "    def __init__(self, dataset, flags, is_test=False, class_ratio=0.125):\n",
        "        self.dataset = dataset\n",
        "        if(is_test):\n",
        "            # self.n = 2590\n",
        "            self.n = 400\n",
        "        else:\n",
        "            self.n = len(dataset)\n",
        "        self.class_ratio = class_ratio\n",
        "        self.flags = flags\n",
        "    \n",
        "    def __iter__(self):\n",
        "        targets = np.array(self.dataset.targets)\n",
        "        ak_idxs = np.where(targets==0)[0]\n",
        "        bcc_idxs = np.where(targets==1)[0]\n",
        "        bkl_idxs = np.where(targets==2)[0]\n",
        "        df_idxs = np.where(targets==3)[0]\n",
        "        mel_idxs = np.where(targets==4)[0]\n",
        "        nv_idxs = np.where(targets==5)[0]\n",
        "        scc_idxs = np.where(targets==6)[0]\n",
        "        vasc_idxs = np.where(targets==7)[0]\n",
        "        ak = np.random.choice(ak_idxs, int(self.n * self.class_ratio), replace=self.flags[0])\n",
        "        bcc = np.random.choice(bcc_idxs, int(self.n * self.class_ratio), replace=self.flags[1])\n",
        "        bkl = np.random.choice(bkl_idxs, int(self.n * self.class_ratio), replace=self.flags[2])\n",
        "        df = np.random.choice(df_idxs, int(self.n * self.class_ratio), replace=self.flags[3])\n",
        "        mel = np.random.choice(mel_idxs, int(self.n * self.class_ratio), replace=self.flags[4])\n",
        "        nv = np.random.choice(nv_idxs, int(self.n * self.class_ratio), replace=self.flags[5])\n",
        "        scc = np.random.choice(scc_idxs, int(self.n * self.class_ratio), replace=self.flags[6])\n",
        "        vasc = np.random.choice(vasc_idxs, int(self.n * self.class_ratio), replace=self.flags[7])\n",
        "        idxs = np.hstack([ak, bcc, bkl, df, mel, nv, scc, vasc])\n",
        "        np.random.shuffle(idxs)\n",
        "        return iter(idxs)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3349288c",
      "metadata": {},
      "outputs": [],
      "source": [
        "class EqualDataLoader(Dataset):\n",
        "    def __init__(self, dataset, class_counts):\n",
        "        self.dataset = dataset\n",
        "        self.class_counts = class_counts\n",
        "        self.class_samples = []\n",
        "        for i in range(len(class_counts)):\n",
        "            self.class_samples.append([j for j in range(len(dataset)) if dataset[j][1] == i])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        class_idx = index % len(self.class_counts)\n",
        "        sample_idx = np.random.choice(self.class_samples[class_idx])\n",
        "        return self.dataset[sample_idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return sum(self.class_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5263fed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5263fed",
        "outputId": "019408ee-9143-4a29-acbe-22a48bbdf8b0"
      },
      "outputs": [],
      "source": [
        "# dataloaders = {x: DataLoader(image_datasets[x], batch_size=hyper_parameter['batch_size'], shuffle=True) for x in sets}\n",
        "\n",
        "# train_flags = [True, True, True, True, True, False, True, True]\n",
        "# test_flags = [True, True, True, True, True, False, True, True]\n",
        "\n",
        "# dataloaders = {\n",
        "#     'train': DataLoader(image_datasets['train'], batch_size=hyper_parameter['batch_size'], sampler=RatioSampler(dataset=image_datasets['train'], flags=train_flags)),\n",
        "#     'test': DataLoader(image_datasets['test'], batch_size=hyper_parameter['batch_size'], sampler=RatioSampler(image_datasets['test'], flags=test_flags, is_test=True))\n",
        "# }\n",
        "\n",
        "# dataloaders = {\n",
        "#     'train': DataLoader(image_datasets['train'], batch_size=hyper_parameter['batch_size'], sampler=RatioSampler(image_datasets['train'], flags=train_flags)),\n",
        "#     'test': DataLoader(image_datasets['test'], batch_size=hyper_parameter['batch_size'], sampler=RatioSampler(image_datasets['test'], flags=test_flags))\n",
        "# }\n",
        "\n",
        "dataloaders = {\n",
        "    'train': DataLoader(EqualDataLoader(dataset=image_datasets['train'], class_counts=train_class_frequency), batch_size=hyper_parameter['batch_size'], sampler=equalitySampler),\n",
        "    'test': DataLoader(image_datasets['test'], batch_size=hyper_parameter['batch_size'], shuffle=True)\n",
        "}\n",
        "\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in sets}\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "print('train: {} test: {}'.format(dataset_sizes['train'], dataset_sizes['test']))\n",
        "print(class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70f424ba",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Loader - count the frequency of images in every iteration\n",
        "num_ak, num_bcc, num_bkl, num_df, num_mel, num_nv, num_scc, num_vasc = 0,0,0,0,0,0,0,0\n",
        "for data, labels in dataloaders['train']:\n",
        "    num_ak += torch.sum(labels==0)\n",
        "    num_bcc += torch.sum(labels==1)\n",
        "    num_bkl += torch.sum(labels==2)\n",
        "    num_df += torch.sum(labels==3)\n",
        "    num_mel += torch.sum(labels==4)\n",
        "    num_nv += torch.sum(labels==5)\n",
        "    num_scc += torch.sum(labels==6)\n",
        "    num_vasc += torch.sum(labels==7)\n",
        "print(f'AK: {num_ak}, BCC: {num_bcc}, BKL: {num_bkl}, DF: {num_df}, MEL: {num_mel}, NV: {num_nv}, SCC: {num_scc}, VASC: {num_vasc}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70f424ba",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Loader - count the frequency of images in every iteration\n",
        "num_ak, num_bcc, num_bkl, num_df, num_mel, num_nv, num_scc, num_vasc = 0,0,0,0,0,0,0,0\n",
        "for data, labels in dataloaders['test']:\n",
        "    num_ak += torch.sum(labels==0)\n",
        "    num_bcc += torch.sum(labels==1)\n",
        "    num_bkl += torch.sum(labels==2)\n",
        "    num_df += torch.sum(labels==3)\n",
        "    num_mel += torch.sum(labels==4)\n",
        "    num_nv += torch.sum(labels==5)\n",
        "    num_scc += torch.sum(labels==6)\n",
        "    num_vasc += torch.sum(labels==7)\n",
        "print(f'AK: {num_ak}, BCC: {num_bcc}, BKL: {num_bkl}, DF: {num_df}, MEL: {num_mel}, NV: {num_nv}, SCC: {num_scc}, VASC: {num_vasc}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JhdiIr8l58JK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "JhdiIr8l58JK",
        "outputId": "4ed47642-5de5-4b28-87de-c75edea207c4"
      },
      "outputs": [],
      "source": [
        "# Helper function for inline image display\n",
        "def imshow(inp, title):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "images, classes = next(iter(dataloaders['train']))\n",
        "\n",
        "# Create a grid from the images and show them\n",
        "img_grid = torchvision.utils.make_grid(images)\n",
        "imshow(img_grid, title=[class_names[x] for x in classes])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eab98e51",
      "metadata": {
        "id": "eab98e51"
      },
      "outputs": [],
      "source": [
        "# class Identity(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(Identity, self).__init__()\n",
        "    \n",
        "#     def forward(self, x):\n",
        "#         return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26c66394",
      "metadata": {
        "id": "26c66394"
      },
      "outputs": [],
      "source": [
        "base_model = [\n",
        "    # expand_ratio, channels, repeats, stride, kernel_size\n",
        "    [1, 16, 1, 1, 3],\n",
        "    [6, 24, 2, 2, 3],\n",
        "    [6, 40, 2, 2, 5],\n",
        "    [6, 80, 3, 2, 3],\n",
        "    [6, 112, 3, 1, 5],\n",
        "    [6, 192, 4, 2, 5],\n",
        "    [6, 320, 1, 1, 3],\n",
        "]\n",
        "\n",
        "phi_values = {\n",
        "    # tuple of: (phi_value, resolution, drop_rate)\n",
        "    \"b0\": (0, 224, 0.2),  # alpha, beta, gamma, depth = alpha ** phi\n",
        "    \"b1\": (0.5, 240, 0.2),\n",
        "    \"b2\": (1, 260, 0.3),\n",
        "    \"b3\": (2, 300, 0.3),\n",
        "    \"b4\": (3, 380, 0.4),\n",
        "    \"b5\": (4, 456, 0.4),\n",
        "    \"b6\": (5, 528, 0.5),\n",
        "    \"b7\": (6, 600, 0.5),\n",
        "}\n",
        "\n",
        "class CNNBlock(nn.Module):\n",
        "    def __init__(\n",
        "            self, in_channels, out_channels, kernel_size, stride, padding, groups=1\n",
        "    ):\n",
        "        super(CNNBlock, self).__init__()\n",
        "        self.cnn = nn.Conv2d(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size,\n",
        "            stride,\n",
        "            padding,\n",
        "            groups=groups,\n",
        "            bias=False,\n",
        "        )\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.silu = nn.SiLU() # SiLU <-> Swish\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.silu(self.bn(self.cnn(x)))\n",
        "\n",
        "class SqueezeExcitation(nn.Module):\n",
        "    def __init__(self, in_channels, reduced_dim):\n",
        "        super(SqueezeExcitation, self).__init__()\n",
        "        self.se = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1), # C x H x W -> C x 1 x 1\n",
        "            nn.Conv2d(in_channels, reduced_dim, 1),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(reduced_dim, in_channels, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * self.se(x)\n",
        "\n",
        "class InvertedResidualBlock(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size,\n",
        "            stride,\n",
        "            padding,\n",
        "            expand_ratio,\n",
        "            reduction=4, # squeeze excitation\n",
        "            survival_prob=0.8, # for stochastic depth\n",
        "    ):\n",
        "        super(InvertedResidualBlock, self).__init__()\n",
        "        self.survival_prob = 0.8\n",
        "        self.use_residual = in_channels == out_channels and stride == 1\n",
        "        hidden_dim = in_channels * expand_ratio\n",
        "        self.expand = in_channels != hidden_dim\n",
        "        reduced_dim = int(in_channels / reduction)\n",
        "\n",
        "        if self.expand:\n",
        "            self.expand_conv = CNNBlock(\n",
        "                in_channels, hidden_dim, kernel_size=3, stride=1, padding=1,\n",
        "            )\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            CNNBlock(\n",
        "                hidden_dim, hidden_dim, kernel_size, stride, padding, groups=hidden_dim,\n",
        "            ),\n",
        "            SqueezeExcitation(hidden_dim, reduced_dim),\n",
        "            nn.Conv2d(hidden_dim, out_channels, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "        )\n",
        "\n",
        "    def stochastic_depth(self, x):\n",
        "        if not self.training:\n",
        "            return x\n",
        "\n",
        "        binary_tensor = torch.rand(x.shape[0], 1, 1, 1, device=x.device) < self.survival_prob\n",
        "        return torch.div(x, self.survival_prob) * binary_tensor\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self.expand_conv(inputs) if self.expand else inputs\n",
        "\n",
        "        if self.use_residual:\n",
        "            return self.stochastic_depth(self.conv(x)) + inputs\n",
        "        else:\n",
        "            return self.conv(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b73578d",
      "metadata": {
        "id": "7b73578d"
      },
      "outputs": [],
      "source": [
        "class EfficientNet(nn.Module):\n",
        "    def __init__(self, version, num_classes):\n",
        "        super(EfficientNet, self).__init__()\n",
        "        width_factor, depth_factor, dropout_rate = self.calculate_factors(version)\n",
        "        last_channels = ceil(1280 * width_factor)\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.features = self.create_features(width_factor, depth_factor, last_channels)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(last_channels, num_classes),\n",
        "        )\n",
        "\n",
        "    def calculate_factors(self, version, alpha=1.2, beta=1.1):\n",
        "        phi, res, drop_rate = phi_values[version]\n",
        "        depth_factor = alpha ** phi\n",
        "        width_factor = beta ** phi\n",
        "        return width_factor, depth_factor, drop_rate\n",
        "\n",
        "    def create_features(self, width_factor, depth_factor, last_channels):\n",
        "        channels = int(32 * width_factor)\n",
        "        features = [CNNBlock(3, channels, 3, stride=2, padding=1)]\n",
        "        in_channels = channels\n",
        "\n",
        "        for expand_ratio, channels, repeats, stride, kernel_size in base_model:\n",
        "            out_channels = 4*ceil(int(channels*width_factor) / 4)\n",
        "            layers_repeats = ceil(repeats * depth_factor)\n",
        "\n",
        "            for layer in range(layers_repeats):\n",
        "                features.append(\n",
        "                    InvertedResidualBlock(\n",
        "                        in_channels,\n",
        "                        out_channels,\n",
        "                        expand_ratio=expand_ratio,\n",
        "                        stride = stride if layer == 0 else 1,\n",
        "                        kernel_size=kernel_size,\n",
        "                        padding=kernel_size//2, # if k=1:pad=0, k=3:pad=1, k=5:pad=2\n",
        "                    )\n",
        "                )\n",
        "                in_channels = out_channels\n",
        "\n",
        "        features.append(\n",
        "            CNNBlock(in_channels, last_channels, kernel_size=1, stride=1, padding=0)\n",
        "        )\n",
        "\n",
        "        return nn.Sequential(*features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.features(x))\n",
        "        return self.classifier(x.view(x.shape[0], -1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eab98e51",
      "metadata": {
        "id": "eab98e51"
      },
      "outputs": [],
      "source": [
        "# class Identity(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(Identity, self).__init__()\n",
        "    \n",
        "#     def forward(self, x):\n",
        "#         return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb0ea140",
      "metadata": {
        "id": "eb0ea140"
      },
      "outputs": [],
      "source": [
        "version = hyper_parameter['version']\n",
        "model = EfficientNet(version=version, num_classes=hyper_parameter['num_classes'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb0ea140",
      "metadata": {
        "id": "eb0ea140"
      },
      "outputs": [],
      "source": [
        "# model = models.efficientnet_b0(weights='DEFAULT')\n",
        "# model.classifier = nn.Sequential(\n",
        "#     nn.Dropout(p=0.2, inplace=True),\n",
        "#     nn.Linear(in_features=1280, out_features=640, bias=True),\n",
        "#     nn.ReLU(),\n",
        "#     nn.Linear(in_features=640, out_features=hyper_parameter['num_classes'], bias=True)\n",
        "#     )\n",
        "# model.classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c6638da",
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c17a4fbb",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "c17a4fbb",
        "outputId": "698c8666-c954-429f-c0d1-ce18c72fdf2e"
      },
      "outputs": [],
      "source": [
        "# define model and optimizers\n",
        "model.to(device)\n",
        "\n",
        "next(model.parameters()).device\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr = hyper_parameter['learning_rate'])\n",
        "\n",
        "# scheduler\n",
        "step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "reduce_on_plateau_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=4, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57904613",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57904613",
        "outputId": "9b040924-f8f2-4e19-ad33-b862bedcea46"
      },
      "outputs": [],
      "source": [
        "# view model architecture\n",
        "# print(model.parameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3m7bUM_72T4F",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3m7bUM_72T4F",
        "outputId": "31698981-2d42-4fbf-fd1f-4fa25da57980"
      },
      "outputs": [],
      "source": [
        "train_results = {\n",
        "    'accuracy': np.zeros(hyper_parameter['no_epochs'], dtype=float),\n",
        "    'balanced_accuracy': np.zeros(hyper_parameter['no_epochs'], dtype=float),\n",
        "    'f1_score': np.zeros(hyper_parameter['no_epochs'], dtype=float),\n",
        "    'loss': np.zeros(hyper_parameter['no_epochs'], dtype=float),\n",
        "    'cf_matrix': np.zeros((hyper_parameter['num_classes'], hyper_parameter['num_classes']), dtype=float)\n",
        "}\n",
        "\n",
        "test_results = {\n",
        "    'accuracy': np.zeros(hyper_parameter['no_epochs'], dtype=float),\n",
        "    'balanced_accuracy': np.zeros(hyper_parameter['no_epochs'], dtype=float),\n",
        "    'f1_score': np.zeros(hyper_parameter['no_epochs'], dtype=float),\n",
        "    'loss': np.zeros(hyper_parameter['no_epochs'], dtype=float),\n",
        "    'cf_matrix': np.zeros((hyper_parameter['num_classes'], hyper_parameter['num_classes']), dtype=float)\n",
        "}\n",
        "\n",
        "results = {\n",
        "    'train': train_results,\n",
        "    'test': test_results\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3fa91e6",
      "metadata": {},
      "outputs": [],
      "source": [
        "save_path = f'{checkpoint_path}.pth.tar'\n",
        "print(f'Save Path: {save_path}')\n",
        "\n",
        "def save_checkpoint(state, epoch, filename = save_path):\n",
        "    print(f'=> Checkpoint at {epoch + 1} saved!')\n",
        "    print(f'Saved at: {save_path}')\n",
        "    torch.save(state, filename)\n",
        "\n",
        "def load_checkpoint(checkpoint):\n",
        "    print(f'=> Loading Checkpoint')\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a08c5f32",
      "metadata": {
        "id": "a08c5f32",
        "tags": [
          "training"
        ]
      },
      "outputs": [],
      "source": [
        "def train_model(model, cirterion, optimizer, scheduler, num_epochs = 1):\n",
        "  since = time.time()\n",
        "\n",
        "  best_model_wts = copy.deepcopy(model.state_dict())\n",
        "  best_acc = 0.0\n",
        "  best_f1_scr = 0.0\n",
        "  # initialize the early stopping counter\n",
        "  early_stopping_counter = 0\n",
        "  early_stopping_threshold = 10\n",
        "  best_test_loss = float('inf')\n",
        "\n",
        "  epoch_preds = {}\n",
        "  epoch_targets = {}\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
        "    print('-' * 15)\n",
        "\n",
        "    for phase in ['train', 'test']:\n",
        "      if phase =='train':\n",
        "        model.train()\n",
        "      else:\n",
        "        model.eval()\n",
        "      \n",
        "      running_loss = 0.0\n",
        "      running_corrects = 0.0\n",
        "\n",
        "      real_targets = []\n",
        "      predicted_targets = []\n",
        "\n",
        "      # Iterate over data.\n",
        "      for inputs, labels in dataloaders[phase]:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward\n",
        "        # track history only if train\n",
        "        with torch.set_grad_enabled(phase == 'train'):\n",
        "          outputs = model(inputs)\n",
        "          _, preds = torch.max(outputs, 1)\n",
        "          \n",
        "          real_targets.extend(labels.detach().cpu().numpy())\n",
        "          predicted_targets.extend(preds.detach().cpu().numpy())\n",
        "\n",
        "          loss = cirterion(outputs, labels)\n",
        "\n",
        "          # backward + optimize only if in train\n",
        "          if phase == 'train':\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "        \n",
        "      ### EPOCH Train & Test ###\n",
        "      # if phase == 'train':\n",
        "      #   scheduler.step()\n",
        "\n",
        "      epoch_f1_score = f1_score(real_targets, predicted_targets, average='micro')\n",
        "      epoch_balanced_acc = balanced_accuracy_score(real_targets, predicted_targets)\n",
        "      epoch_cf_matrix = confusion_matrix(real_targets, predicted_targets)\n",
        "\n",
        "      epoch_loss = running_loss / dataset_sizes[phase]\n",
        "      epoch_acc = running_corrects / dataset_sizes[phase]\n",
        "\n",
        "      if phase == 'test':\n",
        "        epoch_preds[epoch] = predicted_targets\n",
        "        epoch_targets[epoch] = real_targets\n",
        "        scheduler.step()\n",
        "\n",
        "      results[phase]['accuracy'][epoch] = epoch_acc\n",
        "      results[phase]['balanced_accuracy'][epoch] = epoch_balanced_acc\n",
        "      results[phase]['loss'][epoch] = epoch_loss\n",
        "      results[phase]['f1_score'][epoch] = epoch_f1_score\n",
        "      results[phase]['cf_matrix'] += epoch_cf_matrix\n",
        "\n",
        "      # printing epoch resutlts\n",
        "      if (epoch + 1) % 2 == 0:\n",
        "        print(f'{phase} Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}')\n",
        "        print(f'{phase} Balanced Acc: {epoch_balanced_acc:.4f}, F1: {epoch_f1_score:.4f}')\n",
        "\n",
        "      epoch_time = time.time() - since\n",
        "      print(f'Epoch time: {epoch_time // 60:.0f}m {epoch_time %  60:.0f}s')\n",
        "\n",
        "      if phase == 'test' and epoch_acc > best_acc:\n",
        "        best_acc = epoch_acc\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      \n",
        "      if phase == 'test' and epoch_f1_score > best_f1_scr:\n",
        "        best_f1_scr = epoch_f1_score\n",
        "      \n",
        "      if epoch_loss < best_test_loss or epoch_acc > best_acc:\n",
        "        best_test_loss = epoch_loss\n",
        "        best_acc = epoch_acc\n",
        "        early_stopping_counter = 0\n",
        "      else:\n",
        "        early_stopping_counter += 1\n",
        "      # check if the early stopping threshold has been reached\n",
        "      # if early_stopping_counter >= early_stopping_threshold:\n",
        "    \n",
        "    if (epoch + 1) % 2 == 0:\n",
        "      checkpoint = {\n",
        "        'state_dict': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict()\n",
        "      }\n",
        "      save_checkpoint(checkpoint, epoch)\n",
        "\n",
        "    print()\n",
        "    ######################################################\n",
        "\n",
        "  # training Complete\n",
        "  # printing time require to train model\n",
        "  time_elapsed = time.time() - since\n",
        "  print(f'Training complete in {time_elapsed // 60:.0f}m { time_elapsed % 60:.0f}s')\n",
        "\n",
        "  t = time.localtime()\n",
        "  current_time = time.strftime(\"%H:%M\", t)\n",
        "  print(f'Trainig completed at {current_time}')\n",
        "  print(f'Best test Acc: {best_acc:4f}')\n",
        "  print(f'Best F1 : {best_f1_scr:4f}')\n",
        "  print(f\"Balance Acc F1 : {results['test']['balanced_accuracy'][num_epochs-1]:4f}\")\n",
        "\n",
        "  # normalize cf matrix\n",
        "  for phase in ['train', 'test']:\n",
        "    cf_matrix_normalized = results[phase]['cf_matrix'] / results[phase]['cf_matrix'].sum(axis=1, keepdims=True)\n",
        "    df_cm = pd.DataFrame(cf_matrix_normalized, index = class_names, columns = class_names)\n",
        "    plt.figure(figsize = (12,7))\n",
        "    sn.heatmap(df_cm, annot=True)\n",
        "    plt.tight_layout()\n",
        "  plt.savefig(f'{checkpoint_path}_cf_matrix.svg')\n",
        "  plt.show()\n",
        "\n",
        "  for phase in ['train', 'test']:\n",
        "    plt.plot(results[phase]['accuracy'], label='{} accuracy'.format(phase))\n",
        "    plt.title(f'{hyper_parameter[\"model\"]} {hyper_parameter[\"version\"]} learning rate: {hyper_parameter[\"learning_rate\"]} epoch: {hyper_parameter[\"no_epochs\"]}')\n",
        "    plt.legend(bbox_to_anchor = (1.45, 1), loc='upper right')\n",
        "    plt.tight_layout()\n",
        "  plt.savefig(f'{checkpoint_path}_accuracy.svg')\n",
        "  plt.show()\n",
        "\n",
        "  for phase in ['train', 'test']:\n",
        "    plt.plot(results[phase]['balanced_accuracy'], label='{} balanced accuracy'.format(phase))\n",
        "    plt.title(f'{hyper_parameter[\"model\"]} {hyper_parameter[\"version\"]} learning rate: {hyper_parameter[\"learning_rate\"]} epoch: {hyper_parameter[\"no_epochs\"]}')\n",
        "    plt.legend(bbox_to_anchor = (1.45, 1), loc='upper right')\n",
        "    plt.tight_layout()\n",
        "  plt.savefig(f'{checkpoint_path}_balanced-accuracy.svg')\n",
        "  plt.show()\n",
        "\n",
        "  for phase in ['train', 'test']:\n",
        "    plt.plot(results[phase]['f1_score'], label='{} f1 score'.format(phase))\n",
        "    plt.legend(bbox_to_anchor = (1.45, 1), loc='upper right')\n",
        "    plt.tight_layout()\n",
        "  plt.savefig(f'{checkpoint_path}_f1_score.svg')\n",
        "  plt.show()\n",
        "  \n",
        "  for phase in ['train', 'test']:\n",
        "    plt.plot(results[phase]['loss'], label='{} loss'.format(phase))\n",
        "    plt.title(f'{hyper_parameter[\"model\"]} {hyper_parameter[\"version\"]} learning rate: {hyper_parameter[\"learning_rate\"]} epoch: {hyper_parameter[\"no_epochs\"]}')\n",
        "    plt.legend(bbox_to_anchor = (1.45, 1), loc='upper right')\n",
        "    plt.tight_layout()\n",
        "  plt.savefig(f'{checkpoint_path}_loss.svg')\n",
        "  plt.show()\n",
        "\n",
        "  last_epoch = num_epochs - 1\n",
        "  preds = epoch_preds[last_epoch]\n",
        "  targets = epoch_targets[last_epoch]\n",
        "  print('Test Classification Report')\n",
        "  print(classification_report(targets, preds))\n",
        "\n",
        "  # load best model weights\n",
        "  model.load_state_dict(best_model_wts)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9f01377",
      "metadata": {},
      "outputs": [],
      "source": [
        "# load_path = f'checkpoints/efficientnet/efficientnetb0-custom_sampler-loader-full_dataset.pth.tar'\n",
        "# if hyper_parameter['load_model']:\n",
        "#     load_checkpoint(torch.load(load_path))\n",
        "# load_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d844048",
      "metadata": {},
      "outputs": [],
      "source": [
        "# weights = dict()\n",
        "# for name, para in model.named_parameters():\n",
        "#     weights[name] = para\n",
        "# print(weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c61fb1f1",
      "metadata": {
        "tags": [
          "training"
        ]
      },
      "outputs": [],
      "source": [
        "model = train_model(model, criterion, optimizer, step_lr_scheduler, num_epochs=hyper_parameter['no_epochs'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "848e8ab8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export to CSV\n",
        "for phase in ['train', 'test']:\n",
        "    for metric in ['accuracy', 'loss']:\n",
        "        df = pd.DataFrame(results[phase][metric])\n",
        "        df.to_csv(f'E:/CSE499/Skin-Lesion-Classificaiton/output/{hyper_parameter[\"model\"]}-{hyper_parameter[\"version\"]}-{phase}-{metric}-{hyper_parameter[\"attempt\"]}.csv', mode='a')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6562d838",
      "metadata": {},
      "outputs": [],
      "source": [
        "for phase in ['train', 'test']:\n",
        "    csv_path = f'./output/{hyper_parameter[\"model\"]}-{hyper_parameter[\"version\"]}-{phase}-{metric}-{hyper_parameter[\"attempt\"]}.csv'\n",
        "    for metric in ['accuracy', 'loss']:\n",
        "        prev_result = pd.read_csv(csv_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43840d15",
      "metadata": {},
      "outputs": [],
      "source": [
        "for phase in ['train', 'test']:\n",
        "    plt.plot(results[phase]['accuracy'], label='{} accuracy'.format(phase))\n",
        "    plt.title(f'{hyper_parameter[\"model\"]} {hyper_parameter[\"version\"]} learning rate: {hyper_parameter[\"learning_rate\"]} epoch: {hyper_parameter[\"no_epochs\"]}')\n",
        "    plt.legend(bbox_to_anchor = (1.45, 1), loc='upper right')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'accuracy.jpeg')\n",
        "plt.show()\n",
        "\n",
        "for phase in ['train', 'test']:\n",
        "    df_cm = pd.DataFrame(results[phase]['cf_matrix'], index = class_names, columns = class_names)\n",
        "    plt.figure(figsize = (12,7))\n",
        "    sn.heatmap(df_cm, annot=True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'cf_matrix{phase}.jpeg')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "f17d1ccb",
      "metadata": {},
      "source": [
        "# Methods to try\n",
        "https://stackoverflow.com/questions/62319228/number-of-instances-per-class-in-pytorch-dataset"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "f08154012ddadd8e950e6e9e035c7a7b32c136e7647e9b7c77e02eb723a8bedb"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
